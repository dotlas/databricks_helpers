{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7807f803-ee1d-4e2c-9ddb-9284a3df60d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Requirements\n",
    "## Databricks\n",
    "* A Databricks Workspace & Workspace Access Token\n",
    "* At least one runnable cluster within the workspace\n",
    "* Workspace attached to a metastore for Delta Lake\n",
    "## Packages\n",
    "`pandas` for data manipulation and `pydantic` for data modeling.\n",
    "\n",
    "* `pandas < 2.0`\n",
    "* `pydantic < 1.11`\n",
    "\n",
    "## Delta table\n",
    "The table who's column description and tags you want to write/update needs to already exist in your delta lake\n",
    "## Infra\n",
    "A cluster is required to be running on the Databricks workspace from where the Delta lake will be accessed. This cluster will behave as an intermediary to accept connections and data from outside Databricks and add the data into Delta lake.\n",
    "\n",
    "In order to add data to Unity catalog, the cluster must be configured to access Unity Catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "256cfca3-dd69-4b03-b0d3-07504f2ed67a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77998633-02bb-4a93-9e3b-f59273ce2d50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "318bb8c0-456a-41a1-950e-bbd7ab706a12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Initialize the catalog, schema and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46c7d8e8-0cd5-46a1-9e92-a49afc97da08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()\n",
    "\n",
    "dbutils.widgets.text(\"catalog\", \"\") \n",
    "catalog : str = getArgument(\"catalog\")\n",
    "\n",
    "dbutils.widgets.text(\"schema\", \"\")\n",
    "schema: str = getArgument(\"schema\")\n",
    "\n",
    "dbutils.widgets.text(\"table\", \"\")\n",
    "table: str = getArgument(\"table\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Steps ðŸ“Š\n",
    "\n",
    "### 1. Input Pydantic Data Model ðŸ“\n",
    "\n",
    "Initialize your pydantic data model which inherits from pydantic `BaseModel` where you have declared all the column descriptions and tags.\n",
    "\n",
    "### 2. Convert the Pydantic data model to a dataframe ðŸš€\n",
    "\n",
    "Next we convert the data model into a dataframe containing the relevant fields, making it easier to retrieve the needed data.\n",
    "\n",
    "\n",
    "### 3. Update Delta Lake Table ðŸ”„\n",
    "\n",
    "Once you are satisfied with the inferred metadata, apply the updates to your Delta Lake table, and it will be enriched with the new descriptions and tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1357b26-0eb2-4206-877b-b33e4d253d93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create your pydantic data model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdda71da-98a8-432b-8653-21a62a91be56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#initialize your pydantic datamodel class here with the class name as pydantic_date_model which inherits from BaseModel\n",
    "pydantic_data_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "008093c5-05e9-4ad4-aca6-533513d97ed4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## convert the data model to a data frame containing the needed info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8f2e2b1-b113-455e-9dcd-b41169fc384f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_data_dictionary(model: type[BaseModel]) -> pd.DataFrame:\n",
    "    \"\"\"Describe the fields of a pydantic model as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        model (Type[BaseModel]): A pydantic model.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame describing the model.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"field_name\": field,\n",
    "                \"field_title\": field_mf.field_info.title,\n",
    "                \"python_type\": field_type\n",
    "                if \"<class '\" not in (field_type := str(field_mf.type_))\n",
    "                else field_type.split(\"'\")[1],\n",
    "                \"nullable\": field_mf.allow_none,\n",
    "                \"description\": field_mf.field_info.description,\n",
    "                \"tags\": field_mf.field_info.extra.get(\"tags\"),\n",
    "            }\n",
    "            for field, field_mf in model.__fields__.items()\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "df = create_data_dictionary(pydantic_data_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd351621-f1f1-4de3-8238-abc145f0b337",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Write the column description and tags into your table columns in delta lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62d5aad4-bdbe-4810-bc94-ead62383e6ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for column_name, column_title, column_description, tags in zip(\n",
    "    df[\"field_name\"].values,\n",
    "    df[\"field_title\"].values,\n",
    "    df[\"description\"].values,\n",
    "    df[\"tags\"].values,\n",
    "):\n",
    "    \n",
    "\n",
    "    spark.sql(\n",
    "        f\"\"\"Alter table {catalog}.{schema}.{table} alter column {column_name}  comment \"{column_description}\" \"\"\"\n",
    "    )\n",
    "    if tags:\n",
    "        tags_list = \",\".join([f'\"{tag}\"' for tag in tags])\n",
    "        spark.sql(\n",
    "            f\"Alter table {catalog}.{schema}.{table} alter column {column_name} set tags ({tags_list})\"\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1341552303710168,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Delta docs update via Pydantic models",
   "widgets": {
    "catalog": {
     "currentValue": "",
     "nuid": "ed6b375f-569f-41ce-b200-1691ad80e5ae",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "catalog",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "schema": {
     "currentValue": "",
     "nuid": "fd3d78bb-0389-45f5-86c8-920ff8299d58",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "table": {
     "currentValue": "",
     "nuid": "2dd0a2b9-2116-49c3-85c4-537d3d34525d",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "table",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
