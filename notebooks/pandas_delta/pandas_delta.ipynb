{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "122e3c8d-e602-407a-abb7-a5b521ef7057",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Delta Lake I/O with Pandas Dataframes outside Databricks Environment\n",
    "\n",
    "![](https://img.shields.io/badge/Databricks-FF3621.svg?style=for-the-badge&logo=Databricks&logoColor=white)\n",
    "![](https://img.shields.io/badge/Delta-003366.svg?style=for-the-badge&logo=Delta&logoColor=white)\n",
    "![](https://img.shields.io/badge/pandas-150458.svg?style=for-the-badge&logo=pandas&logoColor=white)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dotlas/databricks_helpers/blob/main/notebooks/pandas_delta/pandas_delta.ipynb)\n",
    "\n",
    "\n",
    "In this notebook, we showcase some utility functions built on top of existing third-party open source libraries in Python to read or write Pandas Dataframes **from within or outside a Databricks environment into Delta lake on Databricks**. The Delta lake can exist on [Unity Catalog](https://www.databricks.com/product/unity-catalog), or simply be the `hive_metastore` default. \n",
    "\n",
    "## Requirements\n",
    "\n",
    "### Databricks\n",
    "* A Databricks Workspace & Workspace Access Token\n",
    "* At least one runnable cluster within the workspace\n",
    "* Workspace attached to a metastore for Delta Lake\n",
    "\n",
    "### Packages\n",
    "\n",
    "This process heavily relies on [databricks-sql-python](https://github.com/databricks/databricks-sql-python) library which provides us with a [SQLAlchemy](https://sqlalche.me/) interface to write data. `databricks-sql-python` is an open source Python package maintained by Databricks, and `SQLAlchemy` is used since it is the default ORM wrapper used by the Pandas library\n",
    "\n",
    "\n",
    "* `databricks-sql-connector`\n",
    "* `sqlalchemy == 1.4.41`\n",
    "* `pandas < 2.0`\n",
    "\n",
    "### Infra\n",
    "\n",
    "A cluster is required to be running on the Databricks workspace from where the Delta lake will be accessed. This cluster will behave as an intermediary to accept connections and data from outside Databricks and add the data into Delta lake. \n",
    "\n",
    "> In order to add data to Unity catalog, the cluster must be configured to access `Unity Catalog`\n",
    "\n",
    "![](./assets/unity_catalog_cluster.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86830a84-762c-43c8-98e2-b4b46daf4b80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install pandas databricks-sql-connector sqlalchemy==1.4.41 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88bed7fd-8efa-4b41-bfc0-49d4f1b5f62a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import types as sql_types\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "# databricks imports\n",
    "from databricks import sql as databricks_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "434bb481-e475-4241-9da6-667817a1480e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Setup User Inputs\n",
    "\n",
    "When running this on Databricks, `CLUSTER HTTP PATH` and `WORKSPACE HOSTNAME` can be inferred. When running outside Databricks, you need to start a cluster, and then get these values, copy them over to this notebook when it's run externally and use those as parameters\n",
    "\n",
    "Use `HTTP_PATH` from  within the Cluster configuration page for `CLUSTER HTTP PATH` variable like so:\n",
    "\n",
    "![](https://i.stack.imgur.com/qDotH.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c35434f-ceb4-424d-b550-06d593681818",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Fill up the values for the 3 parameters within the cell below when running this notebook outside a Databricks environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18996dc5-875a-4347-816b-770b8e385597",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if notebook is running inside databricks environment\n",
    "DATABRICKS_ENV = any(\"SPARK\" in k for k in os.environ)\n",
    "\n",
    "if DATABRICKS_ENV:\n",
    "    dbutils.widgets.removeAll()\n",
    "    dbutils.widgets.text(\"WORKSPACE ACCESS TOKEN\", \"\")\n",
    "    dbutils.widgets.text(\"WORKSPACE HOSTNAME\", \"\")\n",
    "    dbutils.widgets.text(\"CLUSTER HTTP PATH\", \"\")\n",
    "\n",
    "# INPUT VALUES HERE\n",
    "\n",
    "# The workspace access token. Usually of the form dapi*******\n",
    "databricks_workspace_access_token: str = (\n",
    "    getArgument(\"WORKSPACE ACCESS TOKEN\")\n",
    "    if DATABRICKS_ENV\n",
    "    else \"<INPUT WORKSPACE ACCESS TOKEN HERE>\"\n",
    ")\n",
    "\n",
    "# server hostname like dbc-xxxx.cloud.databricks.com\n",
    "# do not prefix with https:// or add a / at the end\n",
    "databricks_server_hostname: str = (\n",
    "    getArgument(\"WORKSPACE HOSTNAME\")\n",
    "    if DATABRICKS_ENV\n",
    "    else \"<INPUT WORKSPACE URL HERE>\"\n",
    ")\n",
    "\n",
    "# the http path from the cluster configuration -> JDBC/ODBC tab\n",
    "databricks_cluster_jdbc_http_path: str = (\n",
    "    getArgument(\"CLUSTER HTTP PATH\")\n",
    "    if DATABRICKS_ENV\n",
    "    else \"<INPUT CLUSTER HTTP PATH HERE>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bc3bd88-ceb0-4a2c-8276-79ca5f6ce471",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Infer & Assert Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66eae1b2-0c3b-4be2-a1d8-b5a29a601f93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if DATABRICKS_ENV:\n",
    "    # if notebook is running on databricks environment, then infer parameters\n",
    "    if not databricks_cluster_jdbc_http_path:\n",
    "        # spark works without imports within databricks environment\n",
    "        cluster_id: str = spark.conf.get(\n",
    "            \"spark.databricks.clusterUsageTags.clusterId\",\n",
    "        )  # type: ignore\n",
    "        workspace_id: str = spark.conf.get(\n",
    "            \"spark.databricks.clusterUsageTags.clusterOwnerOrgId\",\n",
    "        )  # type: ignore\n",
    "        databricks_cluster_jdbc_http_path = (\n",
    "            f\"sql/protocolv1/o/{workspace_id}/{cluster_id}\"\n",
    "        )\n",
    "\n",
    "    if not databricks_server_hostname:\n",
    "        databricks_server_hostname = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "assert databricks_workspace_access_token, \"Databricks Workspace Access Token Missing\"\n",
    "assert databricks_server_hostname, \"Databricks Hostname Missing\"\n",
    "assert databricks_cluster_jdbc_http_path, \"Cluster JDBC path Missing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf44e023-6416-42ec-9b7e-cbe1e771518a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Setup Connection\n",
    "\n",
    "We will create a SQLAlchemy engine using the credentials required to connect to the cluster and workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a854e9-a983-49c9-9189-86508b083fe1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "databricks_sqlalchemy_url: str = (\n",
    "    \"databricks://token:\"\n",
    "    + databricks_workspace_access_token\n",
    "    + \"@\"\n",
    "    + databricks_server_hostname\n",
    "    + \"?http_path=\"\n",
    "    + databricks_cluster_jdbc_http_path\n",
    ")\n",
    "\n",
    "databricks_alch_engine: Engine = create_engine(databricks_sqlalchemy_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42369035-2002-40f4-b95d-767c0879c29d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Verify that the connection works by listing catalogs on Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1decf93d-be28-4a59-bec7-e11a2ea19b38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalogs = pd.read_sql(\"show catalogs\", databricks_alch_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59dc2781-b6ae-4725-ab2f-cd65f951cc47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Run Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b61812d3-395e-480f-8a62-20b06ab6e797",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name: str = \"samples\"\n",
    "schema_name: str = \"nyctaxi\"\n",
    "table_name: str = \"trips\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "665b0c87-f1a5-4496-868a-22df48f29c7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df: pd.DataFrame = pd.read_sql(\n",
    "    f\"SELECT * FROM {catalog_name}.{schema_name}.{table_name} limit 100\",\n",
    "    databricks_alch_engine,\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pandas_delta",
   "widgets": {
    "CLUSTER HTTP PATH": {
     "currentValue": "",
     "nuid": "9b33f01f-e642-41f9-bd6c-f06013c3d6c2",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "CLUSTER HTTP PATH",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "WORKSPACE ACCESS TOKEN": {
     "currentValue": "dapid1ad0e60b3f5424e92403a38ab5ea5e7",
     "nuid": "4ef011e5-b2e1-4ffc-8fb8-b09907d809a5",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "WORKSPACE ACCESS TOKEN",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "WORKSPACE HOSTNAME": {
     "currentValue": "",
     "nuid": "c0a80a2b-f1b1-433c-8c44-633a3786b835",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "WORKSPACE HOSTNAME",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
