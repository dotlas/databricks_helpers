{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d08046ea-5af6-4d2e-9bfb-483adbd72f55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Update Clusters & Jobs ♻️\n",
    "\n",
    "## Requirements\n",
    "### Databricks\n",
    "* A Databricks Workspace & Workspace Access Token\n",
    "* At least one runnable cluster within the workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5905cc9b-8d77-442e-b915-b0d9cf8825c4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Update SDK to latest version"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade databricks-sdk -q\n",
    "!pip install loguru -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ca906cb-450b-4806-8717-94603a670cef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aae8da7c-d1f1-4f52-ac7e-28c1b1e686e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.compute import (\n",
    "    ClusterDetails,\n",
    "    UpdateClusterResource,\n",
    "    ListClustersFilterBy,\n",
    "    ClusterSource,\n",
    "    InitScriptInfo,\n",
    ")\n",
    "from databricks.sdk.service.jobs import Job, JobSettings, BaseJob, JobCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ff0f6ad-d92a-4ce2-94f5-a7d3d572d8e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "| Parameter Name                  | Description                                                                                                        | Allowed Values                      |\n",
    "| ------------------------------- | ------------------------------------------------------------------------------------------------------------------ | ----------------------------------- |\n",
    "| `workspace_host`                | The **domain** of the Databricks workspace.                                                                        | `str`                               |\n",
    "| `workspace_token`               | The **token** for accessing the Databricks Workspace API                                                           | `str`                               |\n",
    "| `desired_runtime_version`       | The desired **Databricks Runtime Version** for the updated clusters/job clusters.                                  | `str` [Eg: `\"15.4\"`]                |\n",
    "| `init_scripts_dir`              | Path to the common **directory with init scripts** on a Unity Catalog **Volume**                                   | `str`                               |\n",
    "| `cluster_init_script_files`     | **Filenames** for the scripts to be used when initializing the **clusters**. Use `,` commas to separate files.     | `str` [Eg: `\"S-154.sh, RE-154.sh\"`] |\n",
    "| `job_cluster_init_script_files` | **Filenames** for the scripts to be used when initializing the **job clusters**. Use `,` commas to separate files. | `str` [Eg: `\"S-154.sh, RE-154.sh\"`] |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8551cf68-7418-4596-853a-364ba45e9059",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup Widgets"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()\n",
    "\n",
    "dbutils.widgets.text(\"workspace_host\", \"\")\n",
    "workspace_host: str = getArgument(\"workspace_host\")\n",
    "\n",
    "dbutils.widgets.text(\"workspace_token\", \"\")\n",
    "workspace_token: str = getArgument(\"workspace_token\")\n",
    "\n",
    "dbutils.widgets.text(\"desired_runtime_version\", \"\")\n",
    "desired_runtime_version: str = getArgument(\"desired_runtime_version\")\n",
    "\n",
    "dbutils.widgets.text(\"init_scripts_dir\", \"\")\n",
    "# Validate if directory exists and normalize the path\n",
    "init_scripts_dir: str = str(Path(getArgument(\"init_scripts_dir\")).resolve(strict=True))\n",
    "\n",
    "dbutils.widgets.text(\"cluster_init_script_files\", \"\")\n",
    "cluster_init_script_files: list[str] = [\n",
    "    filename.strip() for filename in getArgument(\"cluster_init_script_files\").split(\",\")\n",
    "]\n",
    "# Validate if files exist and are not empty\n",
    "assert all(\n",
    "    (Path(init_scripts_dir) / file_name).exists()\n",
    "    for file_name in cluster_init_script_files\n",
    "), \"One or more cluster init script files do not exist\"\n",
    "\n",
    "dbutils.widgets.text(\"job_cluster_init_script_files\", \"\")\n",
    "job_cluster_init_script_files: list[str] = [\n",
    "    filename.strip()\n",
    "    for filename in getArgument(\"job_cluster_init_script_files\").split(\",\")\n",
    "]\n",
    "# Validate if files exist and are not empty\n",
    "assert all(\n",
    "    (Path(init_scripts_dir) / file_name).exists()\n",
    "    for file_name in job_cluster_init_script_files\n",
    "), \"One or more job cluster init script files do not exist\"\n",
    "\n",
    "assert all(\n",
    "    [\n",
    "        workspace_host,\n",
    "        workspace_token,\n",
    "        desired_runtime_version,\n",
    "        init_scripts_dir,\n",
    "        cluster_init_script_files,\n",
    "        job_cluster_init_script_files,\n",
    "    ]\n",
    "), \"One or more required parameters for notebook functioning are missing\"\n",
    "\n",
    "logger.info(f\"{workspace_host=}\")\n",
    "logger.info(f\"{desired_runtime_version=}\")\n",
    "logger.info(f\"{init_scripts_dir=}\")\n",
    "logger.info(f\"{cluster_init_script_files=}\")\n",
    "logger.info(f\"{job_cluster_init_script_files=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa39e27a-0385-4b2b-a319-eeed62f96483",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup the workspace client"
    }
   },
   "outputs": [],
   "source": [
    "ws = WorkspaceClient(host=workspace_host, token=workspace_token)\n",
    "logger.info(f\"{ws.get_workspace_id()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a7849d-1c94-4faf-afa1-8444c9213365",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validate Input Runtime"
    }
   },
   "outputs": [],
   "source": [
    "valid_workspace_versions: list[str] = sorted(\n",
    "    list(\n",
    "        set(\n",
    "            [\n",
    "                version_tuple.name.split(\" \")[0]\n",
    "                for version_tuple in ws.clusters.spark_versions().versions\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "logger.info(f\"{len(valid_workspace_versions)=:,}\")\n",
    "\n",
    "assert (\n",
    "    desired_runtime_version in valid_workspace_versions\n",
    "), f\"Invalid {desired_runtime_version=}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "995e3a0e-0aca-4cac-bd6e-baa24078dff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Init Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aaedbf9-3102-406f-9c3b-870e24542702",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Init scripts"
    }
   },
   "outputs": [],
   "source": [
    "def make_init_scripts(init_script_files: list[str]):\n",
    "    return [\n",
    "        InitScriptInfo.from_dict(\n",
    "            {\n",
    "                \"volumes\": {\n",
    "                    \"destination\": str(Path(init_scripts_dir) / file_name),\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        for file_name in init_script_files\n",
    "    ]\n",
    "\n",
    "\n",
    "cluster_init_scripts = make_init_scripts(cluster_init_script_files)\n",
    "job_cluster_init_scripts = make_init_scripts(job_cluster_init_script_files)\n",
    "\n",
    "logger.info(f\"{cluster_init_scripts=}\")\n",
    "logger.info(f\"{job_cluster_init_scripts=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b92429-d1b1-48e5-a305-29c3007c3f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Clusters\n",
    "\n",
    "According to the SDK and REST API documentation:\n",
    "\n",
    "- Clusters created as a result of a job cannot be updated via this endpoint. Only those created either via the `UI` or `API` can be changed.\n",
    "- Those clusters that are `RUNNING` will be `TERMINATED` at the time of update and restart with the new configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18545303-a26d-43e5-b500-39c9ea9c1975",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List all clusters"
    }
   },
   "outputs": [],
   "source": [
    "clusters = list(\n",
    "    ws.clusters.list(\n",
    "        filter_by=ListClustersFilterBy(\n",
    "            cluster_sources=[ClusterSource.API, ClusterSource.UI]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "logger.info(f\"Found {len(clusters)} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69aece1f-8c0c-496a-aa1a-cbc613c69c70",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display clusters as table"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([cluster.as_dict() for cluster in clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6ebd085-4694-409a-96f1-a89eaf42119c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save cluster update parameters"
    }
   },
   "outputs": [],
   "source": [
    "# A dictionary which maps each cluster ID to parameters for the cluster update method\n",
    "cluster_updates = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c860c4d-9a70-42d7-92fb-cb664ac80a7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Updating the Databricks Runtime Version\n",
    "\n",
    "The runtime version is the `cluster.spark_version` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd0f7b31-e0a8-471c-b395-ccf5297e87a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function for retrieving the new runtime version"
    }
   },
   "outputs": [],
   "source": [
    "valid_versions = set(\n",
    "    pd.DataFrame(\n",
    "        [version.as_dict() for version in ws.clusters.spark_versions().versions]\n",
    "    )[\"key\"].tolist()\n",
    ")\n",
    "\n",
    "\n",
    "def get_updated_spark_version_key(\n",
    "    spark_version_key: str, desired_runtime_version: str\n",
    ") -> str:\n",
    "    new_spark_version = re.sub(\n",
    "        r\"^\\d{2}\\.\\d\", desired_runtime_version, spark_version_key\n",
    "    )\n",
    "\n",
    "    if new_spark_version not in valid_versions:\n",
    "        raise ValueError(f\"Could not validate version '{new_spark_version}'\")\n",
    "\n",
    "    return new_spark_version\n",
    "\n",
    "\n",
    "assert (\n",
    "    get_updated_spark_version_key(\"11.3.x-photon-scala2.12\", \"15.4\")\n",
    "    == \"15.4.x-photon-scala2.12\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb3415b-6f28-4500-a480-29b43a2373c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Given a cluster, define the params for updating it's runtime version"
    }
   },
   "outputs": [],
   "source": [
    "def update_cluster_spark_version(cluster: ClusterDetails):\n",
    "    cluster_updates[cluster.cluster_id] = {\n",
    "        **(cluster_updates.get(cluster.cluster_id) or {}),\n",
    "        \"spark_version\": get_updated_spark_version_key(\n",
    "            cluster.spark_version, desired_runtime_version\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "for cluster in clusters:\n",
    "    update_cluster_spark_version(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d802ce3-3004-4d6a-980c-a46c1ed81cbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Update the Init Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d188fd2-b3ae-4409-8870-3ad1dbf6dca9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Given a cluster, update it so that it uses specific init scripts"
    }
   },
   "outputs": [],
   "source": [
    "def update_cluster_init_scripts(\n",
    "    cluster: ClusterDetails, init_scripts: list[InitScriptInfo]\n",
    "):\n",
    "    cluster_updates[cluster.cluster_id] = {\n",
    "        **(cluster_updates.get(cluster.cluster_id) or {}),\n",
    "        \"init_scripts\": init_scripts,\n",
    "    }\n",
    "\n",
    "\n",
    "for cluster in clusters:\n",
    "    update_cluster_init_scripts(cluster, cluster_init_scripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1980665-fc7c-4341-b78f-c4c8b425ce42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Execute the updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c32de4a-0967-495a-bce7-cd45ba442eb2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Use the SDK to make the cluster updates"
    }
   },
   "outputs": [],
   "source": [
    "clusters_to_update = clusters\n",
    "names_for_clusters_that_failed_update = []\n",
    "\n",
    "for cluster in clusters_to_update:\n",
    "    cluster_id = cluster.cluster_id\n",
    "\n",
    "    # Do not update the cluster which is running this notebook\n",
    "    # because it will force a restart\n",
    "    if cluster_id == spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\"):\n",
    "        logger.info(\n",
    "            f\"Skipping cluster: '{cluster.cluster_name}', because it is running this notebook\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    updates = cluster_updates.get(cluster_id)\n",
    "\n",
    "    if updates is None:\n",
    "        continue\n",
    "\n",
    "    update_mask = \",\".join(updates.keys())\n",
    "\n",
    "    try:\n",
    "        ws.clusters.update(\n",
    "            cluster_id=cluster_id,\n",
    "            update_mask=update_mask,\n",
    "            cluster=UpdateClusterResource(**updates),\n",
    "        )\n",
    "        logger.info(f\"Updated cluster: '{cluster.cluster_name}'\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to update cluster: '{cluster.cluster_name}'\")\n",
    "        logger.error(e)\n",
    "        names_for_clusters_that_failed_update.append(cluster.cluster_name)\n",
    "\n",
    "\n",
    "cluster_update_failures = len(names_for_clusters_that_failed_update)\n",
    "cluster_count = len(clusters_to_update)\n",
    "\n",
    "if cluster_update_failures > 0:\n",
    "    cluster_update_failure_message = (\n",
    "        f\"Failed to update {cluster_update_failures} of {cluster_count} cluster(s)\"\n",
    "    )\n",
    "    if cluster_update_failures / len(clusters) >= 0.25:\n",
    "        raise Exception(cluster_update_failure_message)\n",
    "\n",
    "    logger.warning(cluster_update_failure_message)\n",
    "else:\n",
    "    logger.info(f\"Updated all {cluster_count} cluster(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ed4b505-873b-4211-8d62-5caff2de6e21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5eb9996-eb42-408e-8851-13985e039ca4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List all jobs"
    }
   },
   "outputs": [],
   "source": [
    "jobs = list(ws.jobs.list(expand_tasks=True))\n",
    "logger.info(f\"Found {len(jobs)} jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ada4fa8c-43ea-45fb-bb0a-6df15bb2a7a3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display jobs as a table"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([job.as_dict() for job in jobs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "939f4dee-2999-4a05-b247-dbfb6f097b20",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creates a new job with its job clusters on the updated runtime version"
    }
   },
   "outputs": [],
   "source": [
    "def update_job_clusters_spark_version(job: Job | BaseJob) -> Job | BaseJob:\n",
    "    job_clusters = []\n",
    "    for jc in job.settings.job_clusters:\n",
    "        njc = jc.__class__.from_dict(jc.as_dict())\n",
    "        njc.new_cluster.spark_version = get_updated_spark_version_key(\n",
    "            njc.new_cluster.spark_version, desired_runtime_version\n",
    "        )\n",
    "        job_clusters.append(njc)\n",
    "\n",
    "    new_job = job.__class__.from_dict(job.as_dict())\n",
    "    new_job.settings.job_clusters = job_clusters\n",
    "    return new_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c5b17e3-d89e-4a8e-a7b8-5ed714c4a7ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creates a new job with its job clusters using the specified init scripts"
    }
   },
   "outputs": [],
   "source": [
    "def update_job_clusters_init_scripts(\n",
    "    job: Job | BaseJob, init_scripts: list[InitScriptInfo]\n",
    ") -> Job | BaseJob:\n",
    "    job_clusters = []\n",
    "    for jc in job.settings.job_clusters:\n",
    "        njc = jc.__class__.from_dict(jc.as_dict())\n",
    "        njc.new_cluster.init_scripts = init_scripts\n",
    "        job_clusters.append(njc)\n",
    "\n",
    "    new_job = job.__class__.from_dict(job.as_dict())\n",
    "    new_job.settings.job_clusters = job_clusters\n",
    "    return new_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80a63f0b-abce-4a5b-8ad5-a151f9ff35a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Update all jobs"
    }
   },
   "outputs": [],
   "source": [
    "names_for_jobs_that_failed_update = []\n",
    "\n",
    "jobs_to_update = jobs\n",
    "\n",
    "for job in jobs_to_update:\n",
    "    njob = update_job_clusters_spark_version(job)\n",
    "    njob = update_job_clusters_init_scripts(njob, job_cluster_init_scripts)\n",
    "\n",
    "    new_settings = njob.settings.as_dict()\n",
    "    new_settings = {\n",
    "        k: v for k, v in new_settings.items() if k in (\"job_clusters\", \"init_scripts\")\n",
    "    }\n",
    "    new_settings = JobSettings.from_dict(new_settings)\n",
    "\n",
    "    try:\n",
    "        ws.jobs.update(job_id=job.job_id, new_settings=njob.settings)\n",
    "        logger.info(f\"Updated job: '{job.settings.name}'\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to update job: '{job.settings.name}'\")\n",
    "        logger.error(e)\n",
    "        names_for_jobs_that_failed_update.append(job.settings.name)\n",
    "\n",
    "job_update_failures = len(names_for_jobs_that_failed_update)\n",
    "job_count = len(jobs_to_update)\n",
    "\n",
    "if job_update_failures > 0:\n",
    "    job_update_failure_message = (\n",
    "        f\"Failed to update {job_update_failures} of {job_count} job(s)\"\n",
    "    )\n",
    "    if job_update_failures / len(jobs) >= 0.25:\n",
    "        raise Exception(job_update_failure_message)\n",
    "\n",
    "    logger.warning(job_update_failure_message)\n",
    "else:\n",
    "    logger.info(f\"Updated all {job_count} job(s)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "update_job_cluster",
   "widgets": {
    "cluster_init_script_files": {
     "currentValue": "",
     "nuid": "57203fa8-3437-4dad-a7b2-ae12d82f7612",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "cluster_init_script_files",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "cluster_init_script_files",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "desired_runtime_version": {
     "currentValue": "",
     "nuid": "37292731-03a3-4422-9f9b-80d4b9fa8e0d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "desired_runtime_version",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "desired_runtime_version",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "init_scripts_dir": {
     "currentValue": "",
     "nuid": "77a17daf-1ac2-4821-8842-a395e102a92c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "init_scripts_dir",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "init_scripts_dir",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "job_cluster_init_script_files": {
     "currentValue": "",
     "nuid": "5017ecfc-acc5-46d4-83aa-44e7276c4ddf",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "job_cluster_init_script_files",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "job_cluster_init_script_files",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "workspace_host": {
     "currentValue": "",
     "nuid": "300637f4-1b1a-41b0-8507-13c8df6f5e65",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "workspace_host",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "workspace_host",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "workspace_token": {
     "currentValue": "",
     "nuid": "96fd30d3-9fe9-4666-a928-ac1afcdde420",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "workspace_token",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "workspace_token",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
