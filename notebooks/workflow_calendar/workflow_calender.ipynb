{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27fd11cc-a11d-4c9a-9a74-e44f8763a4ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Workflow Calendar üìÜ\n",
    "## Requirements\n",
    "### Databricks\n",
    "* A Databricks Workspace & Workspace Access Token\n",
    "* At least one runnable cluster within the workspace\n",
    "* At least one scheduled job in Databricks workflows\n",
    "\n",
    "### Packages\n",
    "This process relies on a package called `cron-schedule-triggers` which is used to infer the cron-schedule expression. `pandas` for data manipulation and `plotly` for visualization.\n",
    "* <a href=\"https://pypi.org/project/cron-schedule-triggers/#:~:text=Cron%20Schedule%20Triggers%20(CSTriggers)%20is,the%20wild%20to%20choose%20from.\">cron-schedule-triggers</a>\n",
    "* <a href=\"https://pypi.org/project/pandas/\">pandas</a>\n",
    "* <a href=\"https://pypi.org/project/plotly/\">plotly</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "760300fa-6924-47b1-b471-40cc1c990670",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89018793-50c2-432c-8979-8e287a048ccc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Optional, Callable\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "from cstriggers.core.trigger import QuartzCron\n",
    "from datetime import timedelta\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "764722d6-53d7-4d0e-93f6-4aa2ad4154d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Input Data\n",
    "\n",
    "> Provide the date values in `YYYY-MM-DD` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "249ef86e-0d94-47af-a9a1-6eea8f273919",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()\n",
    "\n",
    "dbutils.widgets.text(\"start_date\", \"2023-10-01\")\n",
    "\n",
    "start_date: datetime.datetime = datetime.datetime.strptime(\n",
    "    getArgument(\"start_date\"), \"%Y-%m-%d\"\n",
    ")\n",
    "\n",
    "dbutils.widgets.text(\"end_date\", \"2023-11-05\")\n",
    "\n",
    "end_date: datetime.datetime = datetime.datetime.strptime(\n",
    "    getArgument(\"end_date\"), \"%Y-%m-%d\"\n",
    ")\n",
    "\n",
    "dbutils.widgets.text(\"databricks_url\", \"\")\n",
    "databricks_url: str = getArgument(\"databricks_url\")\n",
    "\n",
    "dbutils.widgets.text(\"databricks_workspace_token\", \"\")\n",
    "databricks_workspace_token: str = getArgument(\"databricks_workspace_token\")\n",
    "\n",
    "headers: dict = {\"Authorization\": f\"Bearer {databricks_workspace_token}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c5b390d-b903-4cec-a8b9-f8855f297f12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_params: dict = {\n",
    "    \"LIST_JOBS_LIMIT\": 100,  # max limit\n",
    "    \"LIST_RUNS_LIMIT\": 25,  # max limit\n",
    "    \"EXPAND_RUNS\": \"true\",\n",
    "    \"EXPAND_TASKS\": \"true\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "131b84b3-735e-46bf-9c0e-5494c55606f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def paginate(\n",
    "    can_paginate: bool,\n",
    "    next_page_token: Optional[str],\n",
    "    url: str,\n",
    "    function_to_call: Callable,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Paginates to the next page if possible\n",
    "    input:\n",
    "        can_paginate [bool]: Boolean info about wheather there is additional info.\n",
    "        next_page_token [str]: Token needed in url query param to paginate to next page.\n",
    "        url [str]: Url used to list the needed info.\n",
    "        function_to_call [Callable]: Function that gets called with the paginated url to paginate further.\n",
    "    output:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if next_page_token:\n",
    "        if \"&page_token\" in url:\n",
    "            url = f\"{url[:url.find('&page_token')]}&page_token={next_page_token}\"\n",
    "        else:\n",
    "            url = f\"{url}&page_token={next_page_token}\"\n",
    "        getAllJobs(url)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps üìä\n",
    "\n",
    "### 1. Fetch Workflows and Runs üèÉ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "This notebook begins by fetching all the [workflows](https://docs.databricks.com/api/workspace/jobs/list) in your Databricks workspace. It also retrieves information about the [runs](https://docs.databricks.com/api/workspace/runs/list) that have occurred within a specified date range, which is provided by the user.\n",
    "\n",
    "### 2. Parse the fetched info üß©\n",
    "Workflows have a schedule which is defined using a `quartz_cron-expression` using which we generate the dates of next runs.\n",
    "\n",
    "### 3. Visualizations üìà\n",
    "\n",
    "The notebook provides three insightful visualizations:\n",
    "\n",
    "- **First Scheduled Run of All Workflows**: Visualizes the first scheduled run of each workflow since the start date.\n",
    "\n",
    "- **Scheduled Runs Between Start and End Date**: Shows all scheduled runs that occurred within the specified date range.\n",
    "\n",
    "- **All Runs Since Start Date with Time Taken**: Displays all runs that have occurred since the start date, plotting them along with their execution time for performance analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f8a1915-b729-448f-a87f-536f0e8e01ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## List workflows \n",
    "#### Fetches all workflows in current workspace and its respective configs\n",
    "<a href=\"https://docs.databricks.com/api/workspace/jobs/list\">API Docs</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07dcd0ca-de2a-474b-be0b-906ebb6d9bd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def getAllJobs(list_jobs_url: str) -> None:\n",
    "    \"\"\"\n",
    "    Fetches all the jobs and metadata about them.\n",
    "    input:\n",
    "        lists_jobs_url [str]: Databricks API used to fetch all the jobs.\n",
    "    output:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(\n",
    "        list_jobs_url,\n",
    "        headers=headers,\n",
    "    )\n",
    "    assert response.status_code == 200\n",
    "\n",
    "    response_data = response.json()\n",
    "\n",
    "    for job in response_data.get(\"jobs\", []):\n",
    "        if job.get(\"settings\", {}).get(\"schedule\"):\n",
    "            jobs[job.get(\"job_id\")] = {\n",
    "                \"name\": job.get(\"settings\", {}).get(\"name\"),\n",
    "                \"quartz_cron_expression\": job.get(\"settings\", {})\n",
    "                .get(\"schedule\", {})\n",
    "                .get(\"quartz_cron_expression\")\n",
    "                .lower(),\n",
    "            }\n",
    "\n",
    "    paginate(\n",
    "        response_data.get(\"has_more\", False),\n",
    "        response_data.get(\"next_page_token\"),\n",
    "        list_jobs_url,\n",
    "        getAllJobs,\n",
    "    )\n",
    "\n",
    "\n",
    "jobs = {}  # holds all jobs' info\n",
    "\n",
    "list_jobs_url: str = (\n",
    "    databricks_url \n",
    "    + \"/api/2.1/jobs/list\" \n",
    "    + f\"?limit={query_params.get('LIST_JOBS_LIMIT')}\" \n",
    "    + f\"&expand_tasks={query_params['EXPAND_TASKS']}\"\n",
    ")\n",
    "\n",
    "getAllJobs(list_jobs_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7996e9d-3e42-4be9-9d8e-5d814d1887f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Parse the fetched data\n",
    "#### Infer the cron expression and calculate the next run.  \n",
    "#### Additionally you can also categorize workflows based on the title, as this category is what determines the colour of the plotted workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcfbf534-33c4-4f04-9157-002ba0858386",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def categorizeWorkflow(workflow_title: str) -> str:\n",
    "    \"\"\"You can add custom grouping logic. as this will be used to\n",
    "    group the workflows, as they will be coloured based on their categories\n",
    "    in the plot.\n",
    "    input:\n",
    "        workflow_title : str\n",
    "    output:\n",
    "        category : str\n",
    "    \"\"\"\n",
    "\n",
    "    category = workflow_title  # add custom logic to categorize the workflow\n",
    "    return category\n",
    "\n",
    "\n",
    "for job_id, job_info in jobs.items():\n",
    "    cron_expression = job_info[\"quartz_cron_expression\"]\n",
    "\n",
    "    cron_obj = QuartzCron(\n",
    "        schedule_string=cron_expression,\n",
    "        start_date=start_date,  # This is the start date based on which the next scheduled run is generated. You can change it as per your needs.\n",
    "    )\n",
    "\n",
    "    next_scheduled_run = cron_obj.next_trigger(isoformat=False)\n",
    "    # print(next_scheduled_run)\n",
    "    jobs[job_id][\"next_scheduled_run\"] = next_scheduled_run\n",
    "    jobs[job_id][\"workflow_category\"] = categorizeWorkflow(jobs[job_id][\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "083dcc0f-f5fd-4a3b-b1ea-71c8cda66492",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Jitter workflows\n",
    "#### Sometimes workflows maybe scheduled too close to each other, this causes them to be too close to each other in the visualization, thus we jitter the workflows slighlty so as to obtain a neat visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92f15423-f544-46f1-b040-9d1e6b96b895",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def jitterPoints(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"If two workflow's have schedules too close to each other\n",
    "    then this function moves them a bit away from each other\n",
    "    so that the visualization is neat\"\"\"\n",
    "    # Initialize a flag to keep track of whether any adjustments were made\n",
    "    adjusted = True\n",
    "    max_iterations = 2  # Set a maximum number of iterations, increase if you have a lot of conflicting workflow schedules.\n",
    "    jitter_minutes = 10  # adjust based on need\n",
    "\n",
    "    iteration = 0\n",
    "    while adjusted and iteration < max_iterations:\n",
    "        adjusted = False\n",
    "\n",
    "        for i in range(1, len(df)):\n",
    "            diff = df[\"start_datetime\"].iloc[i] - df[\"start_datetime\"].iloc[i - 1]\n",
    "\n",
    "            if diff <= timedelta(minutes=10):\n",
    "                # Adjust the start time of the current event\n",
    "                df[\"start_datetime\"].iloc[i] = df[\"start_datetime\"].iloc[\n",
    "                    i - 1\n",
    "                ] + timedelta(minutes=jitter_minutes)\n",
    "                adjusted = True\n",
    "\n",
    "        iteration += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function\n",
    "#### Used to generate X axis tick values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateXAxisTickTexts() -> list:\n",
    "    \"\"\"Helper function used to generate x axis tick values\"\"\"\n",
    "    temp = list(range(1, 13)) + list(range(1, 13)) #12 hour clock entries \n",
    "    temp = temp[-1:] + temp[:-1] #right shifting \n",
    "    for idx in range(len(temp)): #filling the AM/PM value as its a 12 hour format\n",
    "        if idx < len(temp) // 2:\n",
    "            temp[idx] = f\"{temp[idx]} AM\"\n",
    "        else:\n",
    "            temp[idx] = f\"{temp[idx]} PM\"\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ead81d1a-8a7e-45ff-a444-22fc0ae1cc1d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Plot the all the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d3b675a-9a3f-41ec-8551-2900c7616383",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adjust the plot dimensions here\n",
    "PLOT_HEIGHT = 700\n",
    "PLOT_WIDTH = 2000\n",
    "POINT_SIZE = 15\n",
    "\n",
    "events = [\n",
    "    {\n",
    "        \"name\": job_info[\"name\"],\n",
    "        \"start_datetime\": job_info[\"next_scheduled_run\"],\n",
    "        \"workflow_category\": job_info[\"workflow_category\"],\n",
    "    }\n",
    "    for job_info in jobs.values()\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(events)\n",
    "\n",
    "df[\"start_datetime\"] = pd.to_datetime(df[\"start_datetime\"])\n",
    "\n",
    "# Sort DataFrame by 'start_datetime'\n",
    "df.sort_values(by=\"start_datetime\", inplace=True)\n",
    "\n",
    "# jitter closeby points\n",
    "df = jitterPoints(df)\n",
    "\n",
    "\n",
    "# Increase the size of all points by adjusting the marker size\n",
    "point_size = POINT_SIZE  # Adjust the size as needed\n",
    "\n",
    "# Create an interactive scatter plot using Plotly Express\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=df[\"start_datetime\"].dt.hour\n",
    "    + df[\"start_datetime\"].dt.minute / 60\n",
    "    + df[\"start_datetime\"].dt.second / 3600,\n",
    "    y=df[\"start_datetime\"].dt.strftime(\"%Y/%m/%d\"),\n",
    "    # y= df['start_datetime'].dt.strftime('%d-%m-%y'),\n",
    "    color=\"workflow_category\",  # Color points by 'workflow_cateogry' column\n",
    "    hover_name=\"name\",  # Display event name on hover\n",
    "    labels={\"x\": \"Time of Day (12-hour format)\", \"y\": \"Date\"},\n",
    "    title=f\"Workflow's first run since {start_date.strftime('%Y-%m-%d')}\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "\n",
    "\n",
    "# Customize the appearance of the plot\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        tickmode=\"array\",\n",
    "        tickvals=list(range(1, 25)),\n",
    "        ticktext=generateXAxisTickTexts(),\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        tickmode=\"array\",\n",
    "        tickvals=list(\n",
    "            range(\n",
    "                0,\n",
    "                int((df[\"start_datetime\"].iloc[-1] - df[\"start_datetime\"].iloc[0]).days)\n",
    "                + 10,\n",
    "            )\n",
    "        ),\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    legend_title_text=\"Workflow Category\",\n",
    "    height=PLOT_HEIGHT,  # Height of the plot\n",
    "    width=PLOT_WIDTH,  # Width of the plot\n",
    ")\n",
    "\n",
    "# Increase the marker size for all points\n",
    "fig.update_traces(marker=dict(size=point_size))\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a22a7529-72a6-4d56-b183-535e834cb2b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Calculate all the scheduled runs \n",
    "#### using `start_date` and `end_data` we calculate all the scheduled runs within the data range\n",
    "#### Using `cron-schedule-triggers` we calculate all the next scheduled runs since the mentioned `start_date` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "801365b4-09b4-4654-bd1d-018f3b38ae4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_scheduled_runs = []\n",
    "for job_id, job_info in jobs.items():\n",
    "    cron_expression = job_info[\"quartz_cron_expression\"]\n",
    "\n",
    "    cron_obj = QuartzCron(\n",
    "        schedule_string=cron_expression,\n",
    "        start_date=start_date,\n",
    "    )\n",
    "\n",
    "    next_scheduled_run = cron_obj.next_trigger(isoformat=False)\n",
    "    runs = []\n",
    "    while next_scheduled_run <= end_date:\n",
    "        runs.append(next_scheduled_run)\n",
    "        next_scheduled_run = cron_obj.next_trigger(isoformat=False)\n",
    "\n",
    "    for run in runs:\n",
    "        all_scheduled_runs.append(\n",
    "            {\n",
    "                \"name\": jobs[job_id][\"name\"],\n",
    "                \"start_datetime\": run,\n",
    "                \"workflow_category\": jobs[job_id][\"workflow_category\"],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bdc6843-0f7b-472c-a1db-7398497399ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da04d5ee-cf9d-441c-a1d2-95bab6a46eed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adjust the plot dimensions here\n",
    "PLOT_HEIGHT = 700\n",
    "PLOT_WIDTH = 2000\n",
    "POINT_SIZE = 15\n",
    "\n",
    "\n",
    "df = pd.DataFrame(all_scheduled_runs)\n",
    "\n",
    "df[\"start_datetime\"] = pd.to_datetime(df[\"start_datetime\"])\n",
    "\n",
    "# Sort DataFrame by 'start_datetime'\n",
    "df.sort_values(by=\"start_datetime\", inplace=True)\n",
    "\n",
    "# jitter closeby points\n",
    "df = jitterPoints(df)\n",
    "\n",
    "# Increase the size of all points by adjusting the marker size\n",
    "point_size = POINT_SIZE  # Adjust the size as needed\n",
    "\n",
    "# Create an interactive scatter plot using Plotly Express\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=df[\"start_datetime\"].dt.hour\n",
    "    + df[\"start_datetime\"].dt.minute / 60\n",
    "    + df[\"start_datetime\"].dt.second / 3600,\n",
    "    y=df[\"start_datetime\"].dt.strftime(\"%Y/%m/%d\"),\n",
    "    color=\"workflow_category\",  # Color points by 'workflow_category' column\n",
    "    hover_name=\"name\",  # Display event name on hover\n",
    "    labels={\"x\": \"Time of Day (12-hour format)\", \"y\": \"Date\"},\n",
    "    title=f\"All Workflow runs scheduled from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "\n",
    "# Customize the appearance of the plot\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        tickmode=\"array\",\n",
    "        tickvals=list(range(1, 25)),\n",
    "        ticktext=generateXAxisTickTexts(),\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        tickmode=\"array\",\n",
    "        tickvals=list(\n",
    "            range(\n",
    "                0,\n",
    "                int((df[\"start_datetime\"].iloc[-1] - df[\"start_datetime\"].iloc[0]).days)\n",
    "                + 10,\n",
    "            )\n",
    "        ),\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    legend_title_text=\"Workflow category\",\n",
    "    height=PLOT_HEIGHT,  # Height of the plot\n",
    "    width=PLOT_WIDTH,  # Width of the plot\n",
    ")\n",
    "\n",
    "# Increase the marker size for all points\n",
    "fig.update_traces(marker=dict(size=point_size))\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb306f7b-fa82-4949-a4de-b1f77e35d015",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## List workflow runs\n",
    "#### Fetch all workflow runs that have taken place since the mentioned start date. Making sure to parse the necessary info\n",
    "<a href=\"https://docs.databricks.com/api/workspace/jobs/listruns\">API Docs</a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26076b8c-d7d9-4ac4-b398-2f1a587801e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_runs_info = []\n",
    "\n",
    "\n",
    "def getAllRuns(list_runs_url: int) -> None:\n",
    "    \"\"\"\n",
    "    Fetches all the run and metadata about a given workflow.\n",
    "    input:\n",
    "        lists_jobs_url [str]: Databricks API used to fetch all the runs belonging to a given job.\n",
    "    output:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(\n",
    "        list_runs_url,\n",
    "        headers=headers,\n",
    "    )\n",
    "    assert response.status_code == 200\n",
    "\n",
    "    response_data = response.json()\n",
    "    pattern = r\"job_id=([\\w-]+)\"\n",
    "    matched = re.search(pattern, list_runs_url)\n",
    "    job_id = int(matched.group(1))\n",
    "\n",
    "    if \"runs\" in response_data:\n",
    "        for run_info in response_data[\"runs\"]:\n",
    "            if (\n",
    "                \"start_time\" in run_info\n",
    "                and \"end_time\" in run_info\n",
    "                and run_info[\"end_time\"]\n",
    "            ):\n",
    "                all_runs_info.append(\n",
    "                    {\n",
    "                        \"Task\": jobs[job_id][\"name\"],\n",
    "                        \"Start\": datetime.datetime.fromtimestamp(\n",
    "                            run_info[\"start_time\"] / 1000\n",
    "                        ),\n",
    "                        \"Finish\": datetime.datetime.fromtimestamp(\n",
    "                            run_info[\"end_time\"] / 1000\n",
    "                        ),\n",
    "                        \"Duration\": (\n",
    "                            datetime.datetime.fromtimestamp(run_info[\"end_time\"] / 1000)\n",
    "                            - datetime.datetime.fromtimestamp(\n",
    "                                run_info[\"start_time\"] / 1000\n",
    "                            )\n",
    "                        ).total_seconds()\n",
    "                        / 3600,\n",
    "                        \"workflow_category\": jobs[job_id][\"workflow_category\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    paginate(\n",
    "        response_data.get(\"has_more\", False),\n",
    "        response_data.get(\"next_page_token\"),\n",
    "        list_runs_url,\n",
    "        getAllRuns,\n",
    "    )\n",
    "\n",
    "\n",
    "job_ids = list(jobs.keys())\n",
    "list_runs_urls = [\n",
    "    f\"{databricks_url}/api/2.1/jobs/runs/list?job_id={job_id}&limit={query_params.get('LIST_RUNS_LIMIT')}&expand_tasks={query_params.get('EXPAND_RUNS')}&start_time_from={start_date.timestamp()*1000}\"\n",
    "    for job_id in job_ids\n",
    "]\n",
    "\n",
    "for url in list_runs_urls:\n",
    "    getAllRuns(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d88250c7-78d4-4555-97ff-fcccb17aabc0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b8e1b9-01d4-4a76-a746-2e5cdd67315b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adjust accordingly\n",
    "PLOT_HEIGHT = 1500\n",
    "PLOT_WIDTH = 2000\n",
    "\n",
    "runs_df = pd.DataFrame(all_runs_info)\n",
    "\n",
    "runs_df[\"Start\"] = pd.to_datetime(runs_df[\"Start\"])\n",
    "runs_df[\"Finish\"] = pd.to_datetime(runs_df[\"Finish\"])\n",
    "\n",
    "runs_df[\"Duration\"] = (\n",
    "    runs_df[\"Finish\"] - runs_df[\"Start\"]\n",
    ").dt.total_seconds() / 3600  # Duration in hours\n",
    "\n",
    "# Create a new column 'Day' representing the day for each task\n",
    "runs_df[\"Day\"] = runs_df[\"Start\"].dt.date\n",
    "runs_df.head()\n",
    "\n",
    "# Extract task, start, and end dates\n",
    "tasks = runs_df[\"Task\"].tolist()\n",
    "start_dates = runs_df[\"Start\"].tolist()\n",
    "end_dates = runs_df[\"Finish\"].tolist()\n",
    "\n",
    "# Create the Gantt chart\n",
    "fig = ff.create_gantt(\n",
    "    runs_df,\n",
    "    title=\"Task Duration Gantt Chart\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=PLOT_HEIGHT,\n",
    "    width=PLOT_WIDTH,\n",
    "    plot_bgcolor=\"white\",\n",
    "    paper_bgcolor=\"white\",\n",
    "    yaxis=dict(showgrid=True, gridcolor=\"lightgray\"),\n",
    "    xaxis=dict(showgrid=True, gridcolor=\"lightgray\"),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": 0,
       "elementNUID": "da04d5ee-cf9d-441c-a1d2-95bab6a46eed",
       "elementType": "command",
       "guid": "1806ca8a-35e7-4bde-b268-8ae24f5a9614",
       "options": null,
       "position": {
        "height": 8,
        "width": 24,
        "x": 0,
        "y": 8,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 0,
       "elementNUID": "73b8e1b9-01d4-4a76-a746-2e5cdd67315b",
       "elementType": "command",
       "guid": "1c1a7f68-0a81-454d-b94b-6e00aa1fdda2",
       "options": null,
       "position": {
        "height": 17,
        "width": 24,
        "x": 0,
        "y": 16,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 0,
       "elementNUID": "4d3b675a-9a3f-41ec-8551-2900c7616383",
       "elementType": "command",
       "guid": "3badc786-a3b5-43a9-83bc-61236ea1cd0d",
       "options": {
        "autoScaleImg": false,
        "scale": 0,
        "showRunButton": false,
        "showTitle": false,
        "titleAlign": "center"
       },
       "position": {
        "height": 8,
        "width": 24,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "89804740-c7b6-44b4-9c72-2e1c14be2084",
     "origId": 0,
     "title": "Schedule Viz",
     "version": "DashboardViewV1",
     "width": 1440
    }
   ],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1634724413475231,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Jobs Schedule Calendar",
   "widgets": {
    "databricks_url": {
     "currentValue": "",
     "nuid": "1252ccd1-8501-4afb-96d1-fd2d12a60852",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "databricks_url",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "databricks_workspace_token": {
     "currentValue": "",
     "nuid": "7944ddb4-88e5-4041-8773-64bf5327fd25",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "databricks_workspace_token",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "end_date": {
     "currentValue": "2023-11-05",
     "nuid": "dc84215a-1528-4af8-83de-d407d7bcc6ad",
     "widgetInfo": {
      "defaultValue": "2023-11-05",
      "label": null,
      "name": "end_date",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "start_date": {
     "currentValue": "2023-10-01",
     "nuid": "a254d69f-7ac4-4911-b323-5f60de54125b",
     "widgetInfo": {
      "defaultValue": "2023-10-01",
      "label": null,
      "name": "start_date",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
