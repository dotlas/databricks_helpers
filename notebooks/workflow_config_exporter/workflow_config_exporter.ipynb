{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4686ad81-9fd0-4c93-9a48-b11576dc4edf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Backup your Databricks Workflows ðŸ—ƒ\n",
    "\n",
    "## Requirements\n",
    "\n",
    "### Databricks\n",
    "\n",
    "* At least one runnable cluster within the workspace\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "| Parameter Name | Parameter Description | Example Value |\n",
    "| --- | --- | --- |\n",
    "| `backup_file_path` | The file path (prefix) to the destination where the backup file will be stored. **Don't include filename in path**. | `s3://my-databricks-backups/jobs` |\n",
    "\n",
    "\n",
    "### Steps\n",
    "\n",
    "#### Fetch Job Configurations\n",
    "\n",
    "We fetch all the workflows present in your workspace, each fetched workflow config will also contain the individual task config present in the workflow and their respective job cluster configs. [Databricks API documentation](https://docs.databricks.com/api/workspace/jobs/list).  \n",
    "\n",
    "#### Parse Information \n",
    "\n",
    "In this step we parse the obtained config info. The main thing to keep in mind is that the cluster config contains some fields which are populated after the cluster is initialized but will be fetched anyway from step 1, we need to remove this field or else when we use the same config to create the workflow later it will throw an error. You can also add any custom logic here. For example: You can include webhook notification ID to be associated with a workflow you like, You can also associate an existing all-purpose-compute to a workflow that you want, etc.  \n",
    "\n",
    "#### Save Configuration to JSON ðŸ’¾\n",
    "\n",
    "We later save the config to file, if you have a mounted s3 bucket or an azure data lake storage you can direcly specify the path as dbutils will take care of the rest. If you are running the notebook locally then you will need to change the code and use python's inbuilt `open` function to get the task done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b80921-ff93-4b60-8b9d-ad26c4b909c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb9a509f-a4c5-4d06-9d93-1a52c0be1322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "from typing import Optional, Callable\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.jobs import JobSettings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f51bef-dc97-4b08-bf45-c49e11db1076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba8b199-65cc-4dfe-8926-dbc8f28a38b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()\n",
    "dbutils.widgets.text(\"backup_file_path\", \"\")\n",
    "backup_file_path: str = getArgument(\"backup_file_path\")\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "query_params = {\n",
    "    \"LIST_JOBS_LIMIT\": 100,  # max limit\n",
    "    \"EXPAND_TASKS\": \"true\",  # provides the complete config info for each job\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004273ff-e821-415c-b57e-74eccd0b2253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## List workflows \n",
    "\n",
    "Fetches all workflows in current workspace and its respective configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b13f2e7-238b-4a11-9c78-acab6c09f479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jobs: dict[int, dict] = {}\n",
    "\n",
    "# Use the SDK's built-in paginator\n",
    "for job in w.jobs.list(expand_tasks=query_params[\"EXPAND_TASKS\"], limit=query_params[\"LIST_JOBS_LIMIT\"]):\n",
    "    jobs[job.job_id] = job.settings.as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac4ea31-c68f-4e86-9208-403ae6023b08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parse the fetched data\n",
    "\n",
    "This is needed because the cluster config info in each task contains some current workspace specific properties, which are populated after cluster initialization, thus it needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48c33f2-3271-4f1b-a80e-f79ab33535c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_jobs(job_info: JobSettings) -> dict:\n",
    "    \"\"\"\n",
    "    input:\n",
    "        job_info [JobSettings]: JobSettings object from the SDK.\n",
    "    output:\n",
    "        dict : Parsed dictionary.\n",
    "    \"\"\"\n",
    "    job_dict = job_info.as_dict()\n",
    "\n",
    "    for cluster_info in job_dict.get(\"job_clusters\", []):\n",
    "        new_cluster = cluster_info.get(\"new_cluster\", {})\n",
    "        if \"aws_attributes\" in new_cluster:\n",
    "            new_cluster.pop(\"aws_attributes\")\n",
    "\n",
    "    return job_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93df3fd2-b654-419f-a0cf-acac81aedd87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for job_id, job_settings in jobs.items():\n",
    "    parsed = parse_jobs(JobSettings.from_dict(job_settings))\n",
    "    jobs[job_id] = parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84940d82-3c43-4af8-a5a8-54e81712dd31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Backup Job Config\n",
    "\n",
    "Write the obtained config json to disk of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80000619-68c6-4d1f-a234-6c459dc8463c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert len(jobs.keys()) > 1, \"No Jobs Found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe85be21-6d6c-4857-bbf7-bfe52367f30c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "backup_file_path_modded: str = backup_file_path + \"/\" + str(datetime.utcnow().date()).replace(\"-\",\"\") + \".json\"\n",
    "backup_file_path_modded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14159c89-9c1d-4117-bcd6-b36766d869bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "store_flag = None\n",
    "\n",
    "store_flag: bool = dbutils.fs.put(\n",
    "    backup_file_path_modded, json.dumps(jobs), overwrite=False\n",
    ")\n",
    "\n",
    "if not store_flag or store_flag is None:\n",
    "    raise ValueError(\"Unable to Write Jobs Backup\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "workflow_config_exporter",
   "widgets": {
    "backup_file_path": {
     "currentValue": "s3://dotlas-databricks/jobs",
     "nuid": "cbe01358-1720-400b-b9a7-6a1642e1515a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "backup_file_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "backup_file_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
