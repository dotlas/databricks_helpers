{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4686ad81-9fd0-4c93-9a48-b11576dc4edf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Backup your Databricks Workflows ðŸ—ƒ\n",
    "\n",
    "## Requirements\n",
    "\n",
    "### Databricks\n",
    "\n",
    "* A Databricks Workspace & Workspace Access Token\n",
    "* At least one runnable cluster within the workspace\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "| Parameter Name | Parameter Description | Example Value |\n",
    "| --- | --- | --- |\n",
    "| `backup_file_path` | The file path (prefix) to the destination where the backup file will be stored. **Don't include filename in path**. | `s3://my-databricks-backups/jobs` |\n",
    "| `workspace_token` | The workspace token used to interface with Databricks REST API | `dapi****` | \n",
    "| `workspace_url` | The URL of the workspace where the jobs exist, and for which the token is generated for | `https://dbc-***.cloud.databricks.com` |\n",
    "\n",
    "\n",
    "### Steps\n",
    "\n",
    "#### Fetch Job Configurations\n",
    "\n",
    "We fetch all the workflows present in your workspace, each fetched workflow config will also contain the individual task config present in the workflow and their respective job cluster configs. [Databricks API documentation](https://docs.databricks.com/api/workspace/jobs/list).  \n",
    "\n",
    "#### Parse Information \n",
    "\n",
    "In this step we parse the obtained config info. The main thing to keep in mind is that the cluster config contains some fields which are populated after the cluster is initialized but will be fetched anyway from step 1, we need to remove this field or else when we use the same config to create the workflow later it will throw an error. You can also add any custom logic here. For example: You can include webhook notification ID to be associated with a workflow you like, You can also associate an existing all-purpose-compute to a workflow that you want, etc.  \n",
    "\n",
    "#### Save Configuration to JSON ðŸ’¾\n",
    "\n",
    "We later save the config to file, if you have a mounted s3 bucket or an azure data lake storage you can direcly specify the path as dbutils will take care of the rest. If you are running the notebook locally then you will need to change the code and use python's inbuilt `open` function to get the task done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b80921-ff93-4b60-8b9d-ad26c4b909c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb9a509f-a4c5-4d06-9d93-1a52c0be1322",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "from typing import Optional, Callable\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f51bef-dc97-4b08-bf45-c49e11db1076",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a0fc5b-35bc-4873-a965-93ec9c54b5e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()\n",
    "\n",
    "dbutils.widgets.text(\"workspace_url\", \"\")\n",
    "dbutils.widgets.text(\"workspace_token\", \"\")\n",
    "dbutils.widgets.text(\"backup_file_path\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba8b199-65cc-4dfe-8926-dbc8f28a38b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "workspace_url: str = getArgument(\"workspace_url\")\n",
    "workspace_token: str = getArgument(\"workspace_token\")\n",
    "backup_file_path: str = getArgument(\"backup_file_path\")\n",
    "\n",
    "query_params = {\n",
    "    \"LIST_JOBS_LIMIT\": 100,  # max limit\n",
    "    \"EXPAND_TASKS\": \"true\",  # provides the complete config info for each job\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "591794b5-a6a6-4259-a00b-111d04ca51b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51179459-01e4-43cb-bb07-5c42043b6d73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def paginate(\n",
    "    can_paginate: bool,\n",
    "    next_page_token: Optional[str],\n",
    "    url: str,\n",
    "    workspace_token: str,\n",
    "    function_to_call: Callable,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Paginates to the next page if possible\n",
    "    input:\n",
    "        can_paginate [bool]: Boolean info about wheather there is additional info.\n",
    "        next_page_token [str]: Token needed in url query param to paginate to next page.\n",
    "        url [str]: Url used to list the needed info.\n",
    "        function_to_call [Callable]: Function that gets called with the paginated url to paginate further.\n",
    "    output:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if next_page_token and can_paginate:\n",
    "        if \"&page_token\" in url:\n",
    "            url = f\"{url[:url.find('&page_token')]}&page_token={next_page_token}\"\n",
    "        else:\n",
    "            url = f\"{url}&page_token={next_page_token}\"\n",
    "\n",
    "        function_to_call(url, workspace_token)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24b32ea1-dd66-40d0-8579-158b1a18f118",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def getAllJobs(list_jobs_url: str, workspace_token: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches all the jobs and metadata about them.\n",
    "    input:\n",
    "        lists_jobs_url [str]: Databricks API used to fetch all the jobs.\n",
    "        workspace_token [str]: Databricks workspace access token.\n",
    "    output:\n",
    "        None\n",
    "    \"\"\"\n",
    "    response = requests.get(\n",
    "        list_jobs_url,\n",
    "        headers={\"Authorization\": f\"Bearer {workspace_token}\"},\n",
    "    )\n",
    "    assert response.status_code == 200\n",
    "\n",
    "    response_data = response.json()\n",
    "\n",
    "    for job in response_data.get(\"jobs\", []):\n",
    "        jobs[job.get(\"job_id\")] = job.get(\"settings\")\n",
    "\n",
    "    paginate(\n",
    "        can_paginate=response_data.get(\"has_more\", False),\n",
    "        next_page_token=response_data.get(\"next_page_token\"),\n",
    "        url=list_jobs_url,\n",
    "        workspace_token=workspace_token,\n",
    "        function_to_call=getAllJobs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004273ff-e821-415c-b57e-74eccd0b2253",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## List workflows \n",
    "\n",
    "Fetches all workflows in current workspace and its respective configs\n",
    "\n",
    "<a href=\"https://docs.databricks.com/api/workspace/jobs/list\">API Docs</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b13f2e7-238b-4a11-9c78-acab6c09f479",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jobs: dict[int, dict] = {}  # holds all jobs' info\n",
    "\n",
    "list_jobs_url = str(\n",
    "    workspace_url\n",
    "    + \"/api/2.1/jobs/list?\"\n",
    "    + f\"limit={query_params.get('LIST_JOBS_LIMIT')}\"\n",
    "    + f\"&expand_tasks={query_params.get('EXPAND_TASKS')}\"\n",
    ")\n",
    "\n",
    "getAllJobs(list_jobs_url=list_jobs_url, workspace_token=workspace_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac4ea31-c68f-4e86-9208-403ae6023b08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Parse the fetched data\n",
    "\n",
    "This is needed because the cluster config info in each task contains some current workspace specific properties, which are populated after cluster initialization, thus it needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48c33f2-3271-4f1b-a80e-f79ab33535c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parseJobs(job_info: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Modefies the job config.\n",
    "    input:\n",
    "        job_info [dict]: Dict containing all the config info about the job.\n",
    "    output:\n",
    "        dict : parsed result in accordance with the `create job` api payload.\n",
    "    \"\"\"\n",
    "\n",
    "    for cluster_info in job_info.get(\n",
    "        \"job_clusters\", []\n",
    "    ):  # below parsing is same for cluster config payload too.\n",
    "        if \"aws_attributes\" in cluster_info.get(\"new_cluster\"):\n",
    "            cluster_info.get(\"new_cluster\").pop(\"aws_attributes\")\n",
    "\n",
    "    return job_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93df3fd2-b654-419f-a0cf-acac81aedd87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for job_id in jobs.keys():\n",
    "    jobs[job_id] = parseJobs(jobs[job_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84940d82-3c43-4af8-a5a8-54e81712dd31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Backup Job Config\n",
    "\n",
    "Write the obtained config json to disk of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80000619-68c6-4d1f-a234-6c459dc8463c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert len(jobs.keys()) > 1, \"No Jobs Found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe85be21-6d6c-4857-bbf7-bfe52367f30c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "backup_file_path_modded: str = backup_file_path + \"/\" + str(datetime.utcnow().date()).replace(\"-\",\"\") + \".json\"\n",
    "backup_file_path_modded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14159c89-9c1d-4117-bcd6-b36766d869bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "store_flag = None\n",
    "\n",
    "store_flag: bool = dbutils.fs.put(\n",
    "    backup_file_path_modded, json.dumps(jobs), overwrite=False\n",
    ")\n",
    "\n",
    "if not store_flag or store_flag is None:\n",
    "    raise ValueError(\"Unable to Write Jobs Backup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b03f9f52-9a74-423d-acf8-871daedfa95a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "workflow_config_exporter",
   "widgets": {
    "backup_file_path": {
     "currentValue": "",
     "nuid": "cbe01358-1720-400b-b9a7-6a1642e1515a",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "backup_file_path",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "workspace_token": {
     "currentValue": "",
     "nuid": "55eaff73-b759-4823-90f9-f6af6d800bea",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "workspace_token",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "workspace_url": {
     "currentValue": "",
     "nuid": "a8db1735-e9f7-46ac-b856-566a6e477755",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "workspace_url",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
