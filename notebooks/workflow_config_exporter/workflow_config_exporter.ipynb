{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Requirements\n",
    "## Databricks\n",
    "* A Databricks Workspace & Workspace Access Token\n",
    "* At least one runnable cluster within the workspace\n",
    "\n",
    "## Packages\n",
    "This process relies on a package `pandas` for data manipulation.\n",
    "* <a href=\"https://pypi.org/project/pandas/\">pandas</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8b80921-ff93-4b60-8b9d-ad26c4b909c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import necessary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb9a509f-a4c5-4d06-9d93-1a52c0be1322",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "from typing import Optional, Callable\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2eb7ba3-1158-4a8d-be2f-ef5ddb736099",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Steps ðŸ“Š\n",
    "\n",
    "\n",
    "### 1. Fetch Job Configurations ðŸ“¬\n",
    "\n",
    "We fetch all the workflows present in your workspace, each fetched workflow config will also contain the individual task config present in the workflow and their respective job cluster configs. [Databricks API documentation](https://docs.databricks.com/api/workspace/jobs/list).  \n",
    "\n",
    "### 2. Parse Information ðŸ§©\n",
    "\n",
    "In this step we parse the obtained config info. The main thing to keep in mind is that the cluster config contains some fields which are populated after the cluster is initialized but will be fetched anyway from step 1, we need to remove this field or else when we use the same config to create the workflow later it will throw an error. You can also add any custom logic here. For example: You can include webhook notification ID to be associated with a workflow you like, You can also associate an existing all-purpose-compute to a workflow that you want, etc.  \n",
    "\n",
    "### 3. Save Configuration to JSON ðŸ’¾\n",
    "\n",
    "We later save the config to file, if you have a mounted s3 bucket or an azure data lake storage you can direcly specify the path as dbutils will take care of the rest. If you are running the notebook locally then you will need to change the code and use python's inbuilt `open` function to get the task done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03f51bef-dc97-4b08-bf45-c49e11db1076",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Set up workspace urls and access tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a0fc5b-35bc-4873-a965-93ec9c54b5e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()\n",
    "\n",
    "dbutils.widgets.text(\"workspace_url\", \"\")\n",
    "workspace_url: str = getArgument(\"workspace_url\")\n",
    "\n",
    "dbutils.widgets.text(\"workspace_token\", \"\")\n",
    "workspace_token: str = getArgument(\"workspace_token\")\n",
    "\n",
    "dbutils.widgets.text(\"backup_file_path\", \"./databricks_jobs_config.json\")\n",
    "backup_file_path: str = getArgument(\"backup_file_path\")\n",
    "\n",
    "query_params = {\n",
    "    \"LIST_JOBS_LIMIT\": 100,  # max limit\n",
    "    \"EXPAND_TASKS\": \"true\",  # provides the complete config info for each job\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51179459-01e4-43cb-bb07-5c42043b6d73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def paginate(\n",
    "    can_paginate: bool,\n",
    "    next_page_token: Optional[str],\n",
    "    url: str,\n",
    "    workspace_token: str,\n",
    "    function_to_call: Callable,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Paginates to the next page if possible\n",
    "    input:\n",
    "        can_paginate [bool]: Boolean info about wheather there is additional info.\n",
    "        next_page_token [str]: Token needed in url query param to paginate to next page.\n",
    "        url [str]: Url used to list the needed info.\n",
    "        function_to_call [Callable]: Function that gets called with the paginated url to paginate further.\n",
    "    output:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if next_page_token:\n",
    "        if \"&page_token\" in url:\n",
    "            url = f\"{url[:url.find('&page_token')]}&page_token={next_page_token}\"\n",
    "        else:\n",
    "            url = f\"{url}&page_token={next_page_token}\"\n",
    "\n",
    "        getAllJobs(url, workspace_token)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "004273ff-e821-415c-b57e-74eccd0b2253",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## List workflows \n",
    "#### Fetches all workflows in current workspace and its respective configs\n",
    "<a href=\"https://docs.databricks.com/api/workspace/jobs/list\">API Docs</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66fff5ea-0d02-4d06-97fd-1ae7faec163c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def getAllJobs(list_jobs_url: str, workspace_token: str) -> None:\n",
    "    \"\"\"\n",
    "    Fetches all the jobs and metadata about them.\n",
    "    input:\n",
    "        lists_jobs_url [str]: Databricks API used to fetch all the jobs.\n",
    "        workspace_token [str]: Databricks workspace access token.\n",
    "    output:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(\n",
    "        list_jobs_url,\n",
    "        headers={\"Authorization\": f\"Bearer {workspace_token}\"},\n",
    "    )\n",
    "    assert response.status_code == 200\n",
    "\n",
    "    response_data = response.json()\n",
    "\n",
    "    for job in response_data.get(\"jobs\", []):\n",
    "        jobs[job.get(\"job_id\")] = job.get(\"settings\")\n",
    "\n",
    "    paginate(\n",
    "        response_data.get(\"has_more\", False),\n",
    "        response_data.get(\"next_page_token\"),\n",
    "        list_jobs_url,\n",
    "        workspace_token,\n",
    "        getAllJobs,\n",
    "    )\n",
    "\n",
    "\n",
    "jobs = {}  # holds all jobs' info\n",
    "List_jobs_url = f\"{workspace_url}/api/2.1/jobs/list?limit={query_params.get('LIST_JOBS_LIMIT')}&expand_tasks={query_params.get('EXPAND_TASKS')}\"\n",
    "getAllJobs(List_jobs_url, workspace_token)\n",
    "jobs_original = jobs.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ac4ea31-c68f-4e86-9208-403ae6023b08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Parse the fetched data\n",
    "\n",
    "#### This is needed because the cluster config info in each task contains some current workspace specific properties, which are populated after cluster initialization, thus it needs to be removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48c33f2-3271-4f1b-a80e-f79ab33535c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parseJobs(job_info: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Modefies the job config.\n",
    "    input:\n",
    "        job_info [dict]: Dict containing all the config info about the job.\n",
    "    output:\n",
    "        dict : parsed result in accordance with the `create job` api payload.\n",
    "    \"\"\"\n",
    "\n",
    "    for cluster_info in job_info.get(\n",
    "        \"job_clusters\", []\n",
    "    ):  # below parsing is same for cluster config payload too.\n",
    "        if \"aws_attributes\" in cluster_info.get(\"new_cluster\"):\n",
    "            cluster_info.get(\"new_cluster\").pop(\"aws_attributes\")\n",
    "\n",
    "    return job_info\n",
    "\n",
    "\n",
    "for job_id in jobs.keys():\n",
    "    jobs[job_id] = parseJobs(jobs[job_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84940d82-3c43-4af8-a5a8-54e81712dd31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Write to disk\n",
    "#### Write the obtained config json to disk of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14159c89-9c1d-4117-bcd6-b36766d869bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.put(backup_file_path, json.dumps(jobs), overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "jobs config backup notebook",
   "widgets": {
    "backup_file_path": {
     "currentValue": "./databricks_jobs_config.json",
     "nuid": "cbe01358-1720-400b-b9a7-6a1642e1515a",
     "widgetInfo": {
      "defaultValue": "./databricks_jobs_config.json",
      "label": null,
      "name": "backup_file_path",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "workspace_token": {
     "currentValue": "",
     "nuid": "55eaff73-b759-4823-90f9-f6af6d800bea",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "workspace_token",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "workspace_url": {
     "currentValue": "",
     "nuid": "a8db1735-e9f7-46ac-b856-566a6e477755",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "workspace_url",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
